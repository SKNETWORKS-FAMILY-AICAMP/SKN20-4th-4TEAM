#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‹¤íŒ¨Â·ì¬ë„ì „ ì‚¬ë¡€ ë°ì´í„° ìˆ˜ì§‘/ì •ì œ íŒŒì´í”„ë¼ì¸
ì—¬ëŸ¬ PDF ì‚¬ë¡€ì§‘ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import re
import json
import csv
import os
import sys
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
import pymupdf as fitz


@dataclass
class FailureCase:
    """ì‹¤íŒ¨Â·ì¬ë„ì „ ì‚¬ë¡€ ë°ì´í„° êµ¬ì¡°"""
    id: str
    source_pdf: str  # ğŸ‘ˆ ì–´ë–¤ PDFì—ì„œ ì™”ëŠ”ì§€ ì¶”ê°€
    source_title: str
    source_page_range: str
    representative_name: str
    company_name: str
    industry: str
    service_description: str
    founding_year: Optional[str]
    revenue: Optional[str]
    homepage: Optional[str]
    previous_business: str
    first_startup_year: Optional[str]
    closure_year: Optional[str]
    main_failure_reason: str
    sub_failure_reasons: List[str]
    team_issue: str
    funding_issue: str
    mental_impact: str
    recovery_process: str
    pivot_or_retry: str
    support_program: str
    new_approach: str
    key_differentiator: str
    current_achievement: str
    result_after_retry: str
    key_lesson: str
    advice_quote: str
    raw_chunk: str


def load_pdf(pdf_path: str) -> List[tuple]:
    """
    PDF íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
    
    Returns:
        (page_number, page_text) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
    """
    print(f"ğŸ“– PDF ë¡œë”© ì¤‘: {pdf_path}")
    doc = fitz.open(pdf_path)
    pages = []
    
    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    for page_num in range(len(doc)):
        page = doc[page_num]
        text = page.get_text()
        pages.append((page_num + 1, text))
    
    doc.close()
    print(f"âœ… ì´ {len(pages)}í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
    return pages


def preprocess_page_text(text: str) -> str:
    """
    í˜ì´ì§€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
    
    Returns:
        ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r' +', ' ', text)
    
    # 3ê°œ ì´ìƒì˜ ì—°ì† ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ì œê±° (ë‹¨ë… ìˆ«ì ë¼ì¸)
    text = re.sub(r'^\d+$', '', text, flags=re.MULTILINE)
    
    return text.strip()


def extract_company_info(text: str) -> Dict[str, Optional[str]]:
    """
    íšŒì‚¬ ì •ë³´ ë°•ìŠ¤ì—ì„œ ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text: ì‚¬ë¡€ í…ìŠ¤íŠ¸
    
    Returns:
        íšŒì‚¬ ê¸°ë³¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    info = {
        'company_name': None,
        'founding_year': None,
        'homepage': None,
        'service_description': None,
        'revenue': None
    }
    
    # íšŒì‚¬ì„¤ë¦½ ì •ë³´ ì¶”ì¶œ
    founding_match = re.search(r'íšŒì‚¬ì„¤ë¦½\s*[ã€€\s]*([\dë…„ì›”ì¼\s,]+)', text)
    if founding_match:
        founding_text = founding_match.group(1).strip()
        # ì—°ë„ ì¶”ì¶œ
        year_match = re.search(r'(\d{4})ë…„', founding_text)
        if year_match:
            info['founding_year'] = year_match.group(1)
    
    # í™ˆí˜ì´ì§€
    homepage_match = re.search(r'í™ˆí˜ì´ì§€\s*[ã€€\s]*(https?://[^\s]+)', text)
    if homepage_match:
        info['homepage'] = homepage_match.group(1).strip()
    
    # ì£¼ìš”ì‚¬ì—…
    service_match = re.search(r'ì£¼ìš”ì‚¬ì—…\s*[ã€€\s]*(.+?)(?:\n|ë§¤ì¶œì•¡)', text, re.DOTALL)
    if service_match:
        info['service_description'] = service_match.group(1).strip()
    
    # ë§¤ì¶œì•¡
    revenue_match = re.search(r'ë§¤ì¶œì•¡\s*[ã€€\s]*(.+?)(?:\(|$)', text)
    if revenue_match:
        info['revenue'] = revenue_match.group(1).strip()
    
    return info


def classify_industry(service_description: str, company_name: str) -> str:
    """
    ì„œë¹„ìŠ¤ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì—…ì¢…ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    
    Args:
        service_description: ì„œë¹„ìŠ¤ ì„¤ëª…
        company_name: íšŒì‚¬ëª…
    
    Returns:
        ì—…ì¢… ë¶„ë¥˜
    """
    # None ê°’ ì²˜ë¦¬
    service_description = service_description or ""
    company_name = company_name or ""
    
    text = (service_description + " " + company_name).lower()
    
    if any(kw in text for kw in ['êµìœ¡', 'í•™ìŠµ', 'í•™ì›', 'ì—ë“€', 'ì˜¨ë¼ì¸ ë°©ê³¼í›„']):
        return 'ì—ë“€í…Œí¬'
    elif any(kw in text for kw in ['íŒ¨ì…˜', 'ì‡¼í•‘', 'ì˜ë¥˜', 'ì˜·', 'í¸ì§‘ìˆ']):
        return 'íŒ¨ì…˜í…Œí¬'
    elif any(kw in text for kw in ['ì‹í’ˆ', 'ë¨¹ê±°ë¦¬', 'ì‹ì¬ë£Œ', 'ìŒì‹', 'íŒ”ë„ê°']):
        return 'í‘¸ë“œí…Œí¬'
    elif any(kw in text for kw in ['ë†ì—…', 'ìŠ¤ë§ˆíŠ¸íŒœ', 'ì‹ë¬¼', 'ì¬ë°°']):
        return 'ì• ê·¸í…Œí¬'
    elif any(kw in text for kw in ['ë°˜ë ¤ë™ë¬¼', 'í«', 'ì• ì™„ë™ë¬¼']):
        return 'í«í…Œí¬'
    elif any(kw in text for kw in ['ì•±', 'ëª¨ë°”ì¼', 'í”Œë«í¼', 'ì†Œí”„íŠ¸ì›¨ì–´']):
        return 'IT/ì†Œí”„íŠ¸ì›¨ì–´'
    elif any(kw in text for kw in ['iot', 'led', 'ê¸°ìˆ ', 'í•˜ë“œì›¨ì–´']):
        return 'í•˜ë“œì›¨ì–´/IoT'
    else:
        return 'ê¸°íƒ€'


def extract_failure_reasons(text: str) -> tuple:
    """
    ì‹¤íŒ¨ ì›ì¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text: ì‚¬ë¡€ í…ìŠ¤íŠ¸
    
    Returns:
        (main_reason, sub_reasons) íŠœí”Œ
    """
    main_reason = ""
    sub_reasons = []
    
    # ì£¼ìš” ì‹¤íŒ¨ ì›ì¸ íŒ¨í„´
    patterns = [
        r'íì—…ì˜ ê°€ì¥ í° ìš”ì¸ì€[^?]*?\?\s*(.+?)(?:\n\n|Q\.|$)',
        r'íì—…[ì„ë¥¼] [ê²°ì •í•œ|í•˜ê²Œ ëœ] ì´ìœ ëŠ”[^?]*?\?\s*(.+?)(?:\n\n|Q\.|$)',
        r'ê·¸ë§Œ[ë‘ê²Œ|ë’€ê³ ].*?ì´ìœ ëŠ”[^?]*?\?\s*(.+?)(?:\n\n|Q\.|$)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            main_reason = match.group(1).strip()
            # ì²« 1-2ë¬¸ì¥ë§Œ ì¶”ì¶œ
            sentences = re.split(r'[.!?]\s+', main_reason)
            main_reason = '. '.join(sentences[:2]) + '.'
            break
    
    # ë¶€ê°€ì  ì‹¤íŒ¨ ìš”ì¸ í‚¤ì›Œë“œ
    failure_keywords = {
        'ì¤€ë¹„ ë¶€ì¡±': ['ì¤€ë¹„ê°€ ë˜ì–´ ìˆì§€ ì•Š', 'ê²½í—˜ì´ ì—†', 'ë¯¸ìˆ™í–ˆ'],
        'ì‚¬ì—… í™•ì¥ ì‹¤íŒ¨': ['ë¬´ë¦¬í•´ì„œ', 'ê°ë‹¹í•´ì•¼ í•  ì¼', 'ì—­ëŸ‰ì´ ë¶€ì¡±'],
        'ì´ˆì‹¬ ìƒì‹¤': ['ì´ˆì‹¬ ìì²´ê°€ í”ë“¤', 'ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ'],
        'ìˆ˜ìµ ëª¨ë¸ ë¶ˆëª…í™•': ['ìˆ˜ìµ ëª¨ë¸ì´ ëª…í™•í•˜ì§€', 'ì˜ë¯¸ ìˆëŠ” ìˆ˜ìµ'],
        'ì„ íƒê³¼ ì§‘ì¤‘ ì‹¤íŒ¨': ['ì„ íƒê³¼ ì§‘ì¤‘ì„ í•˜ì§€ ëª»'],
        'ì‹œì¥ ë¯¸ìŠ¤ë§¤ì¹˜': ['ì‹œì¥ì—ì„œ ìœ íš¨', 'ê²½ìŸìê°€ ë§'],
    }
    
    for reason, keywords in failure_keywords.items():
        if any(kw in text for kw in keywords):
            sub_reasons.append(reason)
    
    return main_reason, sub_reasons


def extract_issues(text: str) -> tuple:
    """
    íŒ€/ìê¸ˆ ì´ìŠˆë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text: ì‚¬ë¡€ í…ìŠ¤íŠ¸
    
    Returns:
        (team_issue, funding_issue) íŠœí”Œ
    """
    team_issue = "ì—†ìŒ"
    funding_issue = "ì—†ìŒ"
    
    # íŒ€ ì´ìŠˆ
    team_patterns = [
        r'íŒ€ì›.*?(\d+ëª…)',
        r'í•¨ê»˜ ì¼í–ˆë˜ ì‚¬ëŒ',
        r'ì§ë¬´ê°€ ê²¹',
        r'ì¡°ì§ ê´€ë¦¬',
        r'êµ¬ì„±ì›'
    ]
    
    for pattern in team_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            # ê´€ë ¨ ë¬¸ë§¥ ì¶”ì¶œ
            match = re.search(r'([^.]*(?:' + pattern + ')[^.]*\.)', text, re.IGNORECASE)
            if match:
                team_issue = match.group(1).strip()
                break
    
    # ìê¸ˆ ì´ìŠˆ
    funding_patterns = [
        r'ë¹š.*?[ê°šì•˜|ì—†ì—ˆ]',
        r'ëŒ€ì¶œ',
        r'ì§€ì›ë°›ì€ ê¸ˆì•¡',
        r'íˆ¬ì.*?(\d+ì–µ)',
        r'ê¸ˆì „ì .*?[ì§€ì›|íƒ€ê²©|ì–´ë ¤]',
        r'ìê¸ˆ'
    ]
    
    for pattern in funding_patterns:
        match = re.search(r'([^.]*(?:' + pattern + ')[^.]*\.)', text, re.IGNORECASE)
        if match:
            funding_issue = match.group(1).strip()
            break
    
    return team_issue, funding_issue


def extract_key_lesson(text: str) -> str:
    """
    í•µì‹¬ êµí›ˆì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text: ì‚¬ë¡€ í…ìŠ¤íŠ¸
    
    Returns:
        í•µì‹¬ êµí›ˆ
    """
    # í° ë”°ì˜´í‘œë¡œ ê°•ì¡°ëœ êµí›ˆ ë°•ìŠ¤ ì°¾ê¸°
    quote_pattern = r'"([^"]{20,})"'
    quotes = re.findall(quote_pattern, text)
    
    if quotes:
        # ê°€ì¥ ê¸´ ì¸ìš©ë¬¸ì„ ì„ íƒ (ë³´í†µ í•µì‹¬ êµí›ˆ)
        return max(quotes, key=len)
    
    # êµí›ˆ ê´€ë ¨ ì§ˆë¬¸-ë‹µë³€ íŒ¨í„´
    lesson_patterns = [
        r'ì‹¤íŒ¨.*?ì–´ë–¤ ì˜ë¯¸[^?]*?\?\s*(.{50,300}?)(?:\n\n|Q\.|$)',
        r'ë°°ìš´.*?ì [^?]*?\?\s*(.{50,300}?)(?:\n\n|Q\.|$)',
        r'êµí›ˆ[^?]*?\?\s*(.{50,300}?)(?:\n\n|Q\.|$)',
    ]
    
    for pattern in lesson_patterns:
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()
    
    return "ëª…ì‹œì  êµí›ˆ ì—†ìŒ"


def extract_advice_quote(text: str) -> str:
    """
    ì¡°ì–¸/ëª…ì–¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text: ì‚¬ë¡€ í…ìŠ¤íŠ¸
    
    Returns:
        ì¡°ì–¸ ë¬¸êµ¬
    """
    # í° ë”°ì˜´í‘œë¡œ ëœ ì¡°ì–¸ íŒ¨í„´
    quote_pattern = r'"([^"]{15,150})"'
    quotes = re.findall(quote_pattern, text)
    
    # ì¡°ì–¸ì„± í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ì¸ìš©ë¬¸ ìš°ì„ 
    advice_keywords = ['í•´ì•¼', 'í•˜ì„¸ìš”', 'í•˜ë¼', 'ì¶”ì²œ', 'ì¤‘ìš”', 'í•„ìš”']
    
    for quote in quotes:
        if any(kw in quote for kw in advice_keywords):
            return quote
    
    # ê·¸ ì™¸ ì²« ë²ˆì§¸ ì ì ˆí•œ ê¸¸ì´ì˜ ì¸ìš©ë¬¸
    for quote in quotes:
        if 20 < len(quote) < 100:
            return quote
    
    return ""


def split_into_cases(pages: List[tuple]) -> List[Dict]:
    """
    í˜ì´ì§€ë“¤ì„ ì‚¬ë¡€ ë‹¨ìœ„ë¡œ ì²­í‚¹í•©ë‹ˆë‹¤.
    
    Args:
        pages: (page_number, page_text) ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì‚¬ë¡€ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    cases = []
    current_case = None
    current_pages = []
    current_text = []
    
    # Chapter ë˜ëŠ” íšŒì‚¬ëª… íŒ¨í„´
    chapter_pattern = r'Chapter\.(\d+)'
    company_pattern = r'^([ê°€-í£a-zA-Z0-9\s\.]+)$'  # í•œ ì¤„ì— íšŒì‚¬ëª…ë§Œ ìˆëŠ” ê²½ìš°
    
    for page_num, text in pages:
        # í”„ë¡¤ë¡œê·¸, ëª©ì°¨ ë“± ê±´ë„ˆë›°ê¸°
        if page_num < 6:
            continue
        
        # Chapter ì‹œì‘ ê°ì§€
        chapter_match = re.search(chapter_pattern, text)
        is_new_case = False
        
        if chapter_match:
            is_new_case = True
        else:
            # íšŒì‚¬ëª… íŒ¨í„´ ê°ì§€ (í•œ ì¤„ì— 3-20ì ê¸¸ì´ì˜ í…ìŠ¤íŠ¸)
            lines = text.strip().split('\n')
            if lines and len(lines[0]) > 0:
                first_line = lines[0].strip()
                # ì§§ì€ íšŒì‚¬ëª… ê°™ì€ ë‹¨ì¼ ë¼ì¸ ê°ì§€
                if 2 < len(first_line) < 30 and not first_line[0].isdigit():
                    is_new_case = True
        
        if is_new_case:
            # ì´ì „ ì¼€ì´ìŠ¤ ì €ì¥
            if current_case and len(current_text) > 1:  # ìµœì†Œí•œ 2í˜ì´ì§€ ì´ìƒ
                current_case['page_range'] = f"{current_pages[0]}-{current_pages[-1]}"
                current_case['text'] = '\n\n'.join(current_text)
                cases.append(current_case)
            
            # ìƒˆ ì¼€ì´ìŠ¤ ì‹œì‘
            current_case = {
                'chapter': chapter_match.group(1) if chapter_match else 'unknown',
                'start_page': page_num
            }
            current_pages = [page_num]
            current_text = [preprocess_page_text(text)]
        elif current_case:
            # í˜„ì¬ ì¼€ì´ìŠ¤ì— í˜ì´ì§€ ì¶”ê°€
            current_pages.append(page_num)
            current_text.append(preprocess_page_text(text))
    
    # ë§ˆì§€ë§‰ ì¼€ì´ìŠ¤ ì €ì¥
    if current_case and len(current_text) > 1:
        current_case['page_range'] = f"{current_pages[0]}-{current_pages[-1]}"
        current_case['text'] = '\n\n'.join(current_text)
        cases.append(current_case)
    
    print(f"âœ… {len(cases)}ê°œì˜ ì‚¬ë¡€ ì²­í¬ ìƒì„± ì™„ë£Œ")
    return cases


def extract_structured_case(case_dict: Dict, case_index: int, pdf_name: str) -> FailureCase:
    """
    ì‚¬ë¡€ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        case_dict: ì‚¬ë¡€ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        case_index: ì‚¬ë¡€ ì¸ë±ìŠ¤
        pdf_name: PDF íŒŒì¼ëª…
    
    Returns:
        FailureCase ê°ì²´
    """
    text = case_dict['text']
    
    # ì œëª© ì¶”ì¶œ (ì²« ë²ˆì§¸ ì¤„ ë˜ëŠ” Chapter ë‹¤ìŒ ì¤„)
    lines = text.split('\n')
    title = lines[0] if lines else f"ì‚¬ë¡€ {case_index + 1}"
    
    # ëŒ€í‘œìëª… ì¶”ì¶œ
    rep_pattern = r'([ê°€-í£]{2,4})\s*ëŒ€í‘œ'
    rep_match = re.search(rep_pattern, text)
    representative_name = rep_match.group(1) if rep_match else ""
    
    # íšŒì‚¬ëª… ì¶”ì¶œ (ê´„í˜¸ ì•ˆì˜ íšŒì‚¬ëª…)
    company_pattern = r'([ãˆœ(ì£¼)][^\s,]+)'
    company_match = re.search(company_pattern, text)
    company_name = company_match.group(1) if company_match else ""
    
    # íšŒì‚¬ ì •ë³´ ì¶”ì¶œ
    company_info = extract_company_info(text)
    
    # ì—…ì¢… ë¶„ë¥˜
    industry = classify_industry(
        company_info.get('service_description', ''),
        company_name
    )
    
    # ì‹¤íŒ¨ ì›ì¸ ì¶”ì¶œ
    main_failure_reason, sub_failure_reasons = extract_failure_reasons(text)
    
    # ì´ìŠˆ ì¶”ì¶œ
    team_issue, funding_issue = extract_issues(text)
    
    # ì´ì „ ì‚¬ì—… ì¶”ì¶œ
    previous_business = ""
    prev_patterns = [
        r'ì´ì „ì—.*?([^.]{20,100}ì‚¬ì—…)',
        r'ì²˜ìŒ.*?ì°½ì—….*?([^.]{20,100})',
        r'ì²«.*?ì‚¬ì—….*?([^.]{20,100})'
    ]
    for pattern in prev_patterns:
        match = re.search(pattern, text, re.DOTALL)
        if match:
            previous_business = match.group(1).strip()
            break
    
    # ì¬ë„ì „ ë°©ì‹
    pivot_or_retry = "ì‹ ê·œ ì•„ì´í…œ"
    if re.search(r'ê°™ì€|ë™ì¼|ë¹„ìŠ·í•œ.*?ë¶„ì•¼', text, re.IGNORECASE):
        pivot_or_retry = "ë™ì¼ ì—…ì¢… í”¼ë²—"
    elif re.search(r'ì™„ì „íˆ ë‹¤ë¥¸|ìƒˆë¡œìš´ ë¶„ì•¼', text, re.IGNORECASE):
        pivot_or_retry = "ì‹ ê·œ ì—…ì¢… ë„ì „"
    
    # ì§€ì› í”„ë¡œê·¸ë¨
    support_program = ""
    if 'ì¬ë„ì „ ì„±ê³µ íŒ¨í‚¤ì§€' in text or 'ì¬ë„ì „ì„±ê³µíŒ¨í‚¤ì§€' in text:
        support_program = "ì¬ë„ì „ì„±ê³µíŒ¨í‚¤ì§€"
    if 'TIPS' in text or 'íŒìŠ¤' in text:
        support_program += ", TIPS" if support_program else "TIPS"
    
    # ì‹¬ë¦¬ì  ì˜í–¥
    mental_impact = ""
    mental_patterns = [
        r'íì—….*?[ê²°ì •|ì´í›„].*?([^.]{20,100}[ì‹¬ì •|ìƒí™©|ëŠë‚Œ])',
        r'ì‹¬ë¦¬ì .*?([^.]{20,100})',
        r'í˜ë“¤.*?([^.]{20,100})'
    ]
    for pattern in mental_patterns:
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            mental_impact = match.group(1).strip()
            break
    
    # í˜„ì¬ ì„±ê³¼
    current_achievement = ""
    achievement_patterns = [
        r'ì„±ê³¼.*?([^.]{30,150})',
        r'ë§¤ì¶œ.*?(\d+ì–µ)',
        r'íˆ¬ì.*?(\d+ì–µ)',
        r'(\d+ë§Œ).*?[ë‹¤ìš´ë¡œë“œ|íšŒì›]'
    ]
    for pattern in achievement_patterns:
        match = re.search(pattern, text, re.DOTALL)
        if match:
            current_achievement += match.group(0) + ". "
    
    # êµí›ˆ ë° ì¡°ì–¸
    key_lesson = extract_key_lesson(text)
    advice_quote = extract_advice_quote(text)
    
    # FailureCase ê°ì²´ ìƒì„±
    return FailureCase(
        id=f"case_{str(case_index + 1).zfill(3)}",
        source_pdf=pdf_name,  # ğŸ‘ˆ ì¶”ê°€
        source_title=title.strip(),
        source_page_range=case_dict['page_range'],
        representative_name=representative_name,
        company_name=company_name,
        industry=industry,
        service_description=company_info.get('service_description', ''),
        founding_year=company_info.get('founding_year'),
        revenue=company_info.get('revenue'),
        homepage=company_info.get('homepage'),
        previous_business=previous_business,
        first_startup_year=None,
        closure_year=None,
        main_failure_reason=main_failure_reason,
        sub_failure_reasons=sub_failure_reasons,
        team_issue=team_issue,
        funding_issue=funding_issue,
        mental_impact=mental_impact,
        recovery_process="",
        pivot_or_retry=pivot_or_retry,
        support_program=support_program,
        new_approach="",
        key_differentiator="",
        current_achievement=current_achievement.strip(),
        result_after_retry="ì„±ì¥ì¤‘/ì„±ê³µ",
        key_lesson=key_lesson,
        advice_quote=advice_quote,
        raw_chunk=text
    )


def save_as_txt(cases: List[FailureCase], output_path: str):
    """
    TXT í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    (ì‚¬ë¡€ë³„ ìš”ì•½ + ì›ë¬¸ ì²­í¬ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ í˜•íƒœë¡œ ì •ë¦¬)
    
    Args:
        cases: FailureCase ê°ì²´ ë¦¬ìŠ¤íŠ¸
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    if not cases:
        print("âš ï¸ ì €ì¥í•  ì¼€ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    lines = []
    for case in cases:
        lines.append(f"=== {case.id} | {case.company_name or 'íšŒì‚¬ëª… ë¯¸ìƒ'} | {case.source_pdf} ===")  # ğŸ‘ˆ PDFëª… ì¶”ê°€
        lines.append(f"ì œëª©: {case.source_title}")
        lines.append(f"í˜ì´ì§€ ë²”ìœ„: {case.source_page_range}")
        lines.append(f"ëŒ€í‘œì: {case.representative_name or 'ì •ë³´ ì—†ìŒ'}")
        lines.append(f"ì—…ì¢…: {case.industry}")
        lines.append("")
        lines.append(f"[ì£¼ìš” ì‹¤íŒ¨ ì›ì¸]")
        lines.append(case.main_failure_reason or "ì •ë³´ ì—†ìŒ")
        lines.append("")
        lines.append(f"[ë¶€ê°€ ì‹¤íŒ¨ ìš”ì¸]")
        lines.append(", ".join(case.sub_failure_reasons) if case.sub_failure_reasons else "ì •ë³´ ì—†ìŒ")
        lines.append("")
        lines.append(f"[í•µì‹¬ êµí›ˆ]")
        lines.append(case.key_lesson or "ì •ë³´ ì—†ìŒ")
        lines.append("")
        lines.append(f"[ì¡°ì–¸/ì¸ìš©ë¬¸]")
        lines.append(case.advice_quote or "ì •ë³´ ì—†ìŒ")
        lines.append("")
        lines.append(f"[ì›ë¬¸ ì²­í¬]")
        lines.append(case.raw_chunk.strip())
        lines.append("\n" + "-" * 80 + "\n")
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    
    print(f"âœ… TXT ì €ì¥ ì™„ë£Œ: {output_path}")


def process_single_pdf(pdf_path: str) -> List[FailureCase]:
    """
    ë‹¨ì¼ PDFë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
    
    Returns:
        FailureCase ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    pdf_name = os.path.basename(pdf_path)
    print(f"\n{'='*60}")
    print(f"ì²˜ë¦¬ ì¤‘: {pdf_name}")
    print(f"{'='*60}")
    
    # 1. PDF ë¡œë“œ
    pages = load_pdf(pdf_path)
    
    # 2. ì‚¬ë¡€ ì²­í‚¹
    case_chunks = split_into_cases(pages)
    
    # 3. êµ¬ì¡°í™”
    print(f"\nğŸ“Š ì‚¬ë¡€ êµ¬ì¡°í™” ì¤‘...")
    structured_cases = []
    for idx, case_chunk in enumerate(case_chunks):
        print(f"  ì²˜ë¦¬ ì¤‘: Case {idx + 1}/{len(case_chunks)}")
        structured_case = extract_structured_case(case_chunk, idx, pdf_name)
        structured_cases.append(structured_case)
    
    print(f"âœ… {len(structured_cases)}ê°œ ì‚¬ë¡€ êµ¬ì¡°í™” ì™„ë£Œ")
    return structured_cases


def main():
    """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("=" * 60)
    print("ì‹¤íŒ¨Â·ì¬ë„ì „ ì‚¬ë¡€ ë°ì´í„° ìˆ˜ì§‘/ì •ì œ íŒŒì´í”„ë¼ì¸ (ë‹¤ì¤‘ PDF)")
    print("=" * 60)
    
    # ğŸ”¥ ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ (PDF)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    files_to_process = [
        ('failure_case.pdf', 'pdf'),
        ('failure_case2.pdf', 'pdf')
    ]
    
    # íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
    file_paths = []
    for file_name, file_type in files_to_process:
        # 1. ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
        path = os.path.join(script_dir, file_name)
        
        # 2. í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
        if not os.path.exists(path):
            path = os.path.join(os.getcwd(), file_name)
        
        # 3. ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ data í´ë”ì—ì„œ ì°¾ê¸°
        if not os.path.exists(path):
            parent_dir = os.path.dirname(script_dir)
            path = os.path.join(parent_dir, 'data', file_name)
        
        # 4. í”„ë¡œì íŠ¸ rootì˜ data í´ë”ì—ì„œ ì°¾ê¸°
        if not os.path.exists(path):
            project_root = os.path.dirname(os.path.dirname(script_dir))
            path = os.path.join(project_root, 'data', file_name)
        
        if os.path.exists(path):
            file_paths.append((path, file_type))
            print(f"âœ… ë°œê²¬: {file_name} ({file_type.upper()})")
        else:
            print(f"âš ï¸  ì—†ìŒ: {file_name} (ê±´ë„ˆëœ€)")
    
    if not file_paths:
        print(f"\nâŒ ì—ëŸ¬: ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {script_dir}")
        print(f"   í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
        sys.exit(1)
    
    # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
    all_cases = []
    case_counter = 0
    
    for file_path, file_type in file_paths:
        if file_type == 'pdf':
            cases = process_single_pdf(file_path)
        else:
            print(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_type}")
            continue
        
        # ID ì¬í• ë‹¹ (ì „ì²´ í†µí•© ID)
        for case in cases:
            case_counter += 1
            case.id = f"case_{str(case_counter).zfill(3)}"
        
        all_cases.extend(cases)
    
    print(f"\n{'='*60}")
    print(f"âœ¨ ì „ì²´ {len(all_cases)}ê°œ ì‚¬ë¡€ í†µí•© ì™„ë£Œ!")
    print(f"{'='*60}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_dir = os.path.join(project_root, 'data', 'outputs')
    os.makedirs(output_dir, exist_ok=True)
    
    # TXT ì €ì¥
    txt_path = os.path.join(output_dir, 'failure_cases_all.txt')  # ğŸ‘ˆ íŒŒì¼ëª… ë³€ê²½
    save_as_txt(all_cases, txt_path)
    
    print("\n" + "=" * 60)
    print("âœ¨ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“ ì¶œë ¥ ìœ„ì¹˜: {output_dir}")
    print(f"ğŸ“„ íŒŒì¼ëª…: failure_cases_all.txt")
    print("=" * 60)
  
    

if __name__ == "__main__":
    main()