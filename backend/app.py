import os
import warnings
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
import uvicorn

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_community.retrievers import TavilySearchAPIRetriever
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, AIMessage 
from typing import List
from backend.database import save_chat



warnings.filterwarnings("ignore")
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ í™•ì¸
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY ì—†ìŒ! .env í™•ì¸í•´ì¤˜")

TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
if not TAVILY_API_KEY:
    print("âš ï¸ TAVILY_API_KEY ì—†ìŒ - ì›¹ê²€ìƒ‰ ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€")

# ========================================
# FastAPI ì•± ì„¤ì •
# ========================================
app = FastAPI(title="ì°½ì—… ì§€ì› AI ì–´ì‹œìŠ¤í„´íŠ¸ API")

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class ChatRequest(BaseModel):
    question: str
    chat_history: List[dict] = []

class ChatResponse(BaseModel):
    answer: str
    source_type: str  # "internal-rag", "web-search", "fallback"

# ========================================
# ë²¡í„°DB ë° LLM ì´ˆê¸°í™”
# ========================================
print("ğŸ“š ë²¡í„°DB ë¡œë”© ì¤‘...")
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# ë²¡í„°DB ê²½ë¡œ í™•ì¸
vectorstore_path = "./chroma_startup_all"
if not os.path.exists(vectorstore_path):
    print(f"âš ï¸ ê²½ê³ : {vectorstore_path} ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    print("ë²¡í„°DBë¥¼ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")

try:
    vectorstore = Chroma(
        persist_directory=vectorstore_path,
        collection_name="startup_all_rag",
        embedding_function=embedding_model,
    )
    
    all_data = vectorstore.get()
    ids = all_data.get("ids", [])
    print(f"âœ… ë²¡í„°DB ë¡œë“œ ì™„ë£Œ / ì´ ë²¡í„° ê°œìˆ˜: {len(ids)}")
except Exception as e:
    print(f"âŒ ë²¡í„°DB ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ë²¡í„°DB ì—†ì´ ì‹¤í–‰ë©ë‹ˆë‹¤. ì›¹ê²€ìƒ‰ê³¼ Fallbackë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    vectorstore = None

# LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ========================================
# í”„ë¡¬í”„íŠ¸ ì •ì˜
# ========================================

CONTEXTUALIZE_Q_SYSTEM_PROMPT = """
ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë§¤ìš° ì§§ê±°ë‚˜ ë¶ˆì™„ì „í•´ë„, 
ìµœê·¼ ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ í™œìš©í•´ *ì˜ë¯¸ê°€ ëª…í™•í•œ ë…ë¦½ ì§ˆë¬¸*ì„ ë°˜ë“œì‹œ ìƒì„±í•˜ëŠ” ì—­í• ì´ë‹¤.

ê·œì¹™:
1. íˆìŠ¤í† ë¦¬ì— ë“±ì¥í•œ ì£¼ì œ(ì˜ˆ: ì°½ì—… ì§€ì›, ê³µê°„, ì…ì£¼ ìš”ê±´, ì§€ì›ê¸ˆ ë“±)ë¥¼ ë¶„ì„í•œë‹¤.
2. í˜„ì¬ ì§ˆë¬¸ì´ ì§§ê±°ë‚˜ ëª¨í˜¸í•˜ë©´ íˆìŠ¤í† ë¦¬ì—ì„œ ê°€ì¥ ìµœê·¼ ì£¼ì œë¥¼ ìë™ìœ¼ë¡œ ê²°í•©í•´ ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ë§Œë“ ë‹¤.
3. ë‹¨ì–´ 1~4ìì§œë¦¬ ì§ˆë¬¸(ì˜ˆ: 'ì•ˆì–‘ì€?', 'ì°½ì›?')ë„ ë°˜ë“œì‹œ ì™„ì „í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•œë‹¤.
4. ë‹¨ìˆœíˆ ì›ë¬¸ì„ ë˜í’€ì´í•˜ì§€ ì•ŠëŠ”ë‹¤.
5. ì ˆëŒ€ ë‹µë³€í•˜ì§€ ë§ê³ , ë…ë¦½ëœ ì§ˆë¬¸ë§Œ ë°˜í™˜í•œë‹¤.

ì¶œë ¥: ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ë¬¼ì–´ë³´ë ¤ëŠ” ì˜ë„ë¥¼ ì™„ì „íˆ ë“œëŸ¬ë‚´ëŠ” ì§ˆë¬¸ 1ê°œë§Œ ì¶œë ¥.
"""

contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", CONTEXTUALIZE_Q_SYSTEM_PROMPT),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
        ])

# ê´€ë ¨ì„± ê²€ì¦ í”„ë¡¬í”„íŠ¸
relevance_check_prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ë¬¸ì„œì™€ ì§ˆë¬¸ì˜ ê´€ë ¨ì„±ì„ ì—„ê²©í•˜ê²Œ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì§ˆë¬¸]
{question}

[ê²€ìƒ‰ëœ ë¬¸ì„œ ìƒ˜í”Œ]
{documents}

[íŒë‹¨ ê¸°ì¤€]
1. ì§ˆë¬¸ì˜ í•µì‹¬ ì£¼ì œì™€ ë¬¸ì„œì˜ ë‚´ìš©ì´ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ëŠ”ê°€?
2. ë¬¸ì„œê°€ ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€?
3. ë‹¨ìˆœíˆ ìœ ì‚¬í•œ ë‹¨ì–´ê°€ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‹¤ì œ ë‹µë³€ ê°€ëŠ¥í•œ ë‚´ìš©ì¸ê°€?

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€: "ê´€ë ¨ìˆìŒ" ë˜ëŠ” "ê´€ë ¨ì—†ìŒ"

ë‹µë³€:""")

# Query Transformation
qt_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„° ê²€ìƒ‰ì— ì í•©í•œ 'í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ ë¬¸ì¥'ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”.
ë¶ˆí•„ìš”í•œ ë§ì€ ì œê±°í•˜ê³ , í•µì‹¬ ì¡°ê±´ë§Œ ë‚¨ê¸°ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

ë³€í™˜ëœ ê²€ìƒ‰ìš© ë¬¸ì¥:""")

# ë©€í‹°ì¿¼ë¦¬ ìƒì„±
multi_query_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ 3ê°€ì§€ ë‹¤ë¥¸ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ê° ì¿¼ë¦¬ëŠ” í•œ ì¤„ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.
ë²ˆí˜¸ë‚˜ ì„¤ëª… ì—†ì´ ì¿¼ë¦¬ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}""")

# ê¸°ë³¸ RAG í”„ë¡¬í”„íŠ¸
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ì˜ˆë¹„Â·ì´ˆê¸° ì°½ì—…ìë¥¼ ë„ì™€ì£¼ëŠ” 'ì°½ì—… ì§€ì› í†µí•© AI ì–´ì‹œìŠ¤í„´íŠ¸'ì…ë‹ˆë‹¤.

[ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ ìœ í˜•]
- ì§€ì›ì‚¬ì—… ê³µê³  (announcement)
- ì‹¤íŒ¨/ì¬ë„ì „ ì‚¬ë¡€ (cases)
- ì°½ì—… ê³µê°„ ì •ë³´ (space)
- ë²•ë ¹: ì¤‘ì†Œê¸°ì—…ì°½ì—… ì§€ì›ë²• ë“± (law)
- í†µê³„, ë§¤ë‰´ì–¼ ë“± ì°¸ê³  ìë£Œ

[ë‹µë³€ ì›ì¹™]
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ë§¥(Context) ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”.
3. ì§ˆë¬¸ ì„±ê²©ì— ë”°ë¼ ë‹¤ìŒ ì •ë³´ ìœ í˜•ì„ ìš°ì„  í™œìš©í•˜ì„¸ìš”.
   - ì§€ì›ì‚¬ì—…Â·ì‹ ì²­ ê°€ëŠ¥ ì—¬ë¶€ â†’ announcement
   - ë²•ì  ì •ì˜Â·ìê²© ìš”ê±´ â†’ law
   - ì¡°ì–¸Â·ì£¼ì˜ì  â†’ cases
   - ê³µê°„Â·ì…ì£¼ â†’ space
4. í•µì‹¬ ë‹µë³€ í›„ í•„ìš”í•˜ë©´ bulletë¡œ ì •ë¦¬í•˜ì„¸ìš”.
5. ë§ˆì§€ë§‰ì— ì°¸ê³  ê·¼ê±° ìœ í˜•ì„ ìš”ì•½í•˜ì„¸ìš”.
"""),
    ("human", "[ë¬¸ë§¥]\n{context}\n\n[ì§ˆë¬¸]\n{question}\n\n[ë‹µë³€]")
])

# ë²•ë ¹ ì „ìš© í”„ë¡¬í”„íŠ¸
law_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ì¤‘ì†Œê¸°ì—…ì°½ì—… ì§€ì›ë²•ì„ ë°”íƒ•ìœ¼ë¡œ ì°½ì—… ì œë„ì™€ ìš”ê±´ì„ ì„¤ëª…í•˜ëŠ” AIì…ë‹ˆë‹¤.

[ê·œì¹™]
1. ë°˜ë“œì‹œ ë¬¸ë§¥ì— ìˆëŠ” ë²•ë ¹ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ê°€ëŠ¥í•˜ë©´ ì¡°ë¬¸ ë²ˆí˜¸(ì œâ—‹ì¡°)ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.
3. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë²•ë ¹ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì€ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
4. ë‹µë³€ ëì— "â€» ë³¸ ë‹µë³€ì€ ì¼ë°˜ ì •ë³´ ì œê³µì´ë©°, êµ¬ì²´ì ì¸ ë²•ë¥  ìë¬¸ì€ ì•„ë‹™ë‹ˆë‹¤."ë¥¼ í¬í•¨í•˜ì„¸ìš”.
"""),
    ("human", "[ë²•ë ¹ ë¬¸ë§¥]\n{context}\n\n[ì§ˆë¬¸]\n{question}\n\n[ì„¤ëª…]")
])

# ì§€ì›ì‚¬ì—… ì¶”ì²œ í”„ë¡¬í”„íŠ¸
recommend_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ì˜ˆë¹„Â·ì´ˆê¸° ì°½ì—…ìì—ê²Œ ê°€ì¥ ì í•©í•œ ì§€ì›ì‚¬ì—…ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.

[ëª©í‘œ]
ì‚¬ìš©ìì˜ ì¡°ê±´(ë‚˜ì´, ì§€ì—­, ì—…ì¢…, ì°½ì—… ë‹¨ê³„ ë“±)ì„ ê¸°ì¤€ìœ¼ë¡œ
'ì‹¤ì§ˆì ì¸ ë„ì›€ì´ ë˜ëŠ” ì‚¬ì—…(ìê¸ˆÂ·ê³µê°„Â·R&DÂ·ì‹œì œí’ˆÂ·êµìœ¡)'ì„ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤.

[ì¶”ì²œ ìš°ì„ ìˆœìœ„]
1. í˜„ê¸ˆì„± ì§€ì›(ì‚¬ì—…í™” ìê¸ˆ, ì‹œì œí’ˆ ì œì‘ë¹„, R&D)
2. ì…ì£¼ ê³µê°„, ì¥ë¹„ ì§€ì›
3. ì•¡ì…€ëŸ¬ë ˆì´íŒ…, ë©˜í† ë§
4. ë‹¨ìˆœ êµìœ¡/íŠ¹ê°•ì€ ë§ˆì§€ë§‰ ìˆœìœ„

[ì¶”ì²œ ê·œì¹™]
1. ë°˜ë“œì‹œ announcement ë¬¸ì„œë§Œ ì‚¬ìš©
2. ì‚¬ìš©ì ì¡°ê±´ê³¼ 'ì§€ì—­Â·ì—°ë ¹Â·ë‹¨ê³„Â·ì—…ì¢…'ì´ ëª…í™•íˆ ë§ëŠ” ê²ƒë§Œ ì¶”ì²œ
3. ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ ì¶”ì²œ
4. ì¡°ê±´ì´ ë§ëŠ” ì‚¬ì—…ì´ ì—†ìœ¼ë©´ ì†”ì§í•˜ê²Œ ë§í•˜ê¸°
5. ITÂ·ì„œë¹„ìŠ¤ì—…ì´ë©´ 'ê¸°ìˆ Â·ì½˜í…ì¸ Â·í”Œë«í¼' í‚¤ì›Œë“œ í¬í•¨ ì‚¬ì—… ìš°ì„ 

[ì¶œë ¥ í˜•ì‹]
â–ª ì¶”ì²œ ì‚¬ì—…ëª…
â–ª ì™œ ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œì§€
â–ª ì§€ì› ë‚´ìš©(ìê¸ˆ/ê³µê°„/êµìœ¡ ì¤‘ ë¬´ì—‡ì¸ì§€ ëª…í™•íˆ)
â–ª ì‹ ì²­ ëŒ€ìƒ ìš”ì•½
â–ª ì ‘ìˆ˜ ê¸°ê°„
â–ª ì£¼ì˜ì‚¬í•­

ë§ˆì§€ë§‰ ì¤„: [ì°¸ê³ : ì§€ì›ì‚¬ì—… ê³µê³ ]
"""),
    ("human", "[ì§€ì›ì‚¬ì—… ë¬¸ë§¥]\n{context}\n\n[ì‚¬ìš©ì ì¡°ê±´]\n{question}\n\nìœ„ í˜•ì‹ì— ë§ì¶° ì¶”ì²œí•´ ì£¼ì„¸ìš”.")
])

# Fallback í”„ë¡¬í”„íŠ¸
fallback_prompt = ChatPromptTemplate.from_template("""
ì§ˆë¬¸: {question}

ë‚´ë¶€ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€:""")

# ì²´ì¸ ìƒì„±
qt_chain = qt_prompt | llm | StrOutputParser()
multi_query_chain = multi_query_prompt | llm | StrOutputParser()
relevance_chain = relevance_check_prompt | llm | StrOutputParser()
fallback_chain = fallback_prompt | llm | StrOutputParser()
contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()

# ========================================
# í—¬í¼ í•¨ìˆ˜
# ========================================

def choose_prompt(question: str):
    """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ ì„ íƒ"""
    recommend_keywords = ["ì¶”ì²œ", "ë§ëŠ”", "ì‹ ì²­í•  ìˆ˜ ìˆëŠ”", "ì§€ì›í•´ì£¼ëŠ”", 
                         "ì‚¬ì—… ì•Œë ¤ì¤˜", "í˜œíƒ", "ì§€ì›ê¸ˆ", "ì§€ì›ì‚¬ì—…"]
    law_keywords = ["ì •ì˜", "ìê²©", "ìš”ê±´", "ì§€ì›ë²•", "ë²•ì—ì„œ", "ë²•ìƒ", "ì œë„", "ì‹œí–‰", "ê·œì •"]
    
    if any(k in question for k in recommend_keywords):
        return recommend_prompt, "recommend_prompt"
    if any(k in question for k in law_keywords):
        return law_prompt, "law_prompt"
    return rag_prompt, "rag_prompt"

def format_docs_as_context(docs):
    """RAG í”„ë¡¬í”„íŠ¸ì— ë„£ê¸° ì¢‹ì€ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¶œì²˜ ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    if not docs:
        return ""
    parts = []
    for i, d in enumerate(docs, 1):
        if isinstance(d, Document):
            src = d.metadata.get("source", d.metadata.get("url", "unknown"))
            parts.append(f"[ë¬¸ì„œ {i}] (ì¶œì²˜: {src})\n{d.page_content}")
    return "\n\n---\n\n".join(parts)

def search_documents(queries, k_per_query=10):
    """vectorstoreì—ì„œ ë©€í‹°ì¿¼ë¦¬ ê²€ìƒ‰ í›„ ì¤‘ë³µì œê±° â†’ (Document, similarity) ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    if vectorstore is None:
        return []
    
    all_docs_with_scores = []
    seen_contents = set()

    for q in queries:
        try:
            docs_with_scores = vectorstore.similarity_search_with_score(q, k=k_per_query)
        except Exception as e:
            print("âš ï¸ vectorstore.similarity_search_with_score ì˜¤ë¥˜:", e)
            docs_with_scores = []

        for doc, distance in docs_with_scores:
            if doc.page_content in seen_contents:
                continue
            seen_contents.add(doc.page_content)
            similarity = max(0.0, 1.0 - (distance / 2.0))
            all_docs_with_scores.append((doc, similarity))
            
    return all_docs_with_scores

def filter_by_similarity(docs_with_scores, threshold=0.3):
    return [(doc, sim) for doc, sim in docs_with_scores if sim >= threshold]

def check_relevance(question, docs_with_scores):
    """LLMì—ê²Œ ìƒìœ„ Nê°œ ë¬¸ì„œë¡œ ê´€ë ¨ì„± ë¬¼ì–´ë³´ê¸° -> True/False ë°˜í™˜"""
    top_docs = docs_with_scores[:5]
    if not top_docs:
        return False
    docs_text = "\n\n---\n\n".join(
        f"[ë¬¸ì„œ {i+1}] (ì¶œì²˜: {doc.metadata.get('source', 'unknown')})\n{doc.page_content[:600]}"
        for i, (doc, _) in enumerate(top_docs)
    )
    try:
        res = relevance_chain.invoke({"question": question, "documents": docs_text})
        return "ê´€ë ¨ìˆìŒ" in res
    except Exception as e:
        print(f"âš ï¸ ê´€ë ¨ì„± ê²€ì¦ ì˜¤ë¥˜: {e}")
        return False

def web_search(query: str, k=3):
    """Tavily API ê¸°ë°˜ ì›¹ê²€ìƒ‰"""
    if not TAVILY_API_KEY:
        raise RuntimeError("TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        retriever = TavilySearchAPIRetriever(k=k)
        results = retriever.invoke(query) 

        docs = []
        for r in results:
            if isinstance(r, Document):
                docs.append(r)
            elif isinstance(r, dict):
                content = r.get("content") or r.get("snippet") or r.get("title") or str(r)
                docs.append(Document(
                    page_content=content,
                    metadata={"source": "web", "url": r.get("url", "unknown")}
                ))
            else:
                docs.append(Document(
                    page_content=str(r),
                    metadata={"source": "web"}
                ))
        return docs
        
    except Exception as e:
        raise RuntimeError(f"Tavily ì›¹ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

def rag_answer_from_docs(question: str, documents):
    """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ RAG ì‘ë‹µ ìƒì„±"""
    docs = []
    for item in documents:
        if isinstance(item, tuple):
            docs.append(item[0])
        else:
            docs.append(item)
    
    context = format_docs_as_context(docs)
    if not context.strip():
        print("âš ï¸ context ë¹„ì–´ìˆìŒ -> fallback LLMìœ¼ë¡œ ì´ë™")
        return fallback_chain.invoke({"question": question})

    prompt, pname = choose_prompt(question)
    print(f"[í”„ë¡¬í”„íŠ¸ ì‚¬ìš©] {pname} (ë¬¸ì„œ ê¸°ë°˜)")
    
    try:
        answer = (prompt | llm | StrOutputParser()).invoke({
            "context": context,
            "question": question
        })
        return answer
    except Exception as e:
        print(f"âš ï¸ RAG ìƒì„± ì˜¤ë¥˜: {e}")
        return fallback_chain.invoke({"question": question})


# ========================================
# ë©”ì¸ RAG í•¨ìˆ˜
# ========================================
def multi_query_rag_with_qt(question: str, chat_history: List[dict], top_k=10, similarity_threshold=0.3):
    """
    ì „ì²´ íë¦„:
      1) ë…ë¦½ ì§ˆë¬¸ ìƒì„± (Contextualize)
      2) ì¿¼ë¦¬ íŠ¸ëœìŠ¤í¼ (QT) - ë‚´ë¶€ RAGìš©
      3) ë©€í‹°ì¿¼ë¦¬ ìƒì„± (MQ)
      4) ë²¡í„°ê²€ìƒ‰
      5) ìµœì¢… ë¶„ê¸°: ë‚´ë¶€ RAG vs. ì›¹ê²€ìƒ‰/Fallback
      
    Returns:
        tuple: (answer, source_type)
    """
    lc_chat_history = []
    for msg in chat_history :
        if msg.get('role')== 'user':
            lc_chat_history.append(HumanMessage(content=msg['content']))
        elif msg.get('role') == 'assistant':
            lc_chat_history.append(AIMessage(content=msg['content']))

    # 1. ë…ë¦½ ì§ˆë¬¸ ìƒì„± (Contextualize) - LLMì˜ ìµœì¢… ë‹µë³€ ìƒì„± ë° ì›¹ ê²€ìƒ‰ì— ì‚¬ìš©
    try:
        standalone_question = contextualize_q_chain.invoke({
            'chat_history' : lc_chat_history,
            'input' : question
        })
    except Exception as e:
        print(f'ì§ˆë¬¸ ì¬êµ¬ì„± ì˜¤ë¥˜: {e}. ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©.')
        standalone_question = question

    print(f'[íˆìŠ¤í† ë¦¬] ë…ë¦½ ì§ˆë¬¸: {standalone_question}')

    # 2. ë‚´ë¶€ RAG ê²€ìƒ‰ìš© Query Transform (QT) 
    try:
        qt_result = qt_chain.invoke({"question": standalone_question})

        if isinstance(qt_result, str):
            rag_qt_query = qt_result
        elif hasattr(qt_result, "content"):
            rag_qt_query = qt_result.content
        elif isinstance(qt_result, dict):
            rag_qt_query = qt_result.get("text") or qt_result.get("output")
        else:
            rag_qt_query = standalone_question

    except Exception as e:
        print(f"[QT ERROR] {e}")
        rag_qt_query = standalone_question

    print(f"[QT] ë³€í™˜ (ë‚´ë¶€ RAG): {rag_qt_query}")
    
    # 3. ë©€í‹° ì¿¼ë¦¬ (MQ) - ë‚´ë¶€ RAG ê²€ìƒ‰ì— ì‚¬ìš©
    try:
        mq_text = multi_query_chain.invoke({"question": rag_qt_query})
        queries = [line.strip() for line in mq_text.splitlines() if line.strip()]
    except Exception as e:
        queries = [rag_qt_query]
    print(f"[ë©€í‹°ì¿¼ë¦¬] {len(queries)}ê°œ: {queries}")

    # 4. Vector search (ë©€í‹°ì¿¼ë¦¬)
    all_docs = search_documents(queries)
    print(f"[ê²€ìƒ‰] ì´ {len(all_docs)}ê°œ ë¬¸ì„œ í›„ë³´ í™•ë³´")

    # 5. ìœ ì‚¬ë„ í•„í„°ë§
    filtered_docs = filter_by_similarity(all_docs, similarity_threshold)
    print(f"[1ì°¨ í•„í„°ë§] ìœ ì‚¬ë„ >={similarity_threshold}: {len(filtered_docs)}ê°œ")

    is_relevant = False
    
    if filtered_docs:
        print("[2ì°¨ í•„í„°ë§] LLM ê´€ë ¨ì„± ê²€ì¦ ì¤‘...")
        is_relevant = check_relevance(standalone_question, filtered_docs)

    # 6. ìµœì¢… ë¶„ê¸°: ë‚´ë¶€ RAG vs. ì›¹ ê²€ìƒ‰/Fallback
    if is_relevant:
        print("âœ… ë‚´ë¶€ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆìŒ â†’ ë‚´ë¶€ RAG ì‹¤í–‰")
        useful = filtered_docs[:top_k]
        # ìµœì¢… ë‹µë³€ ìƒì„±ì—ëŠ” íˆìŠ¤í† ë¦¬ ë°˜ì˜ëœ ë…ë¦½ ì§ˆë¬¸ ì‚¬ìš©
        answer = rag_answer_from_docs(standalone_question, useful)
        return answer, "internal-rag"
    else:
        print("âš ï¸ ë‚´ë¶€ ë¬¸ì„œ (ì—†ê±°ë‚˜/ë¬´ê´€) â†’ ì›¹ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜")
        
        # 6-1. ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”: standalone_questionì„ qt_chainì— ì¬í™œìš©í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ ê°œì„ 
        try:
            web_search_query = qt_chain.invoke({"question": standalone_question})
        except Exception as e:
            print(f"âš ï¸ ì›¹ ì¿¼ë¦¬ ë³€í™˜ ì˜¤ë¥˜: {e}")
            web_search_query = standalone_question
        print(f"[ì›¹QT/ì¬í™œìš©] ë³€í™˜: {web_search_query}")

        try:
            # ìµœì í™”ëœ ì¿¼ë¦¬ë¥¼ ì›¹ ê²€ìƒ‰ì— ì‚¬ìš©
            web_docs = web_search(web_search_query)
        except Exception as e:
            print(f"âš ï¸ ì›¹ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            answer = fallback_chain.invoke({"question": standalone_question})
            return answer, "fallback"

        if not web_docs:
            print("âš ï¸ ì›¹ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ LLM ìì²´ì§€ì‹ìœ¼ë¡œ ì‘ë‹µ")
            answer = fallback_chain.invoke({"question": standalone_question})
            return answer, "fallback"
        
        answer = rag_answer_from_docs(standalone_question, web_docs)
        return answer, "web-search"


# ========================================
# API ì—”ë“œí¬ì¸íŠ¸
# ========================================
@app.get("/")
async def root():
    return {
        "message": "ì°½ì—… ì§€ì› AI ì–´ì‹œìŠ¤í„´íŠ¸ APIê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.",
        "vectordb_loaded": vectorstore is not None,
        "tavily_enabled": bool(TAVILY_API_KEY)
    }

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        question = request.question.strip()
        chat_history = request.chat_history

        if not question:
            raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        print(f"\n{'='*60}")
        print(f"[API ìš”ì²­] {question}")
        print(f"[API ìš”ì²­] íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(chat_history)}")
        print(f"{'='*60}")

        # ğŸ”‘ ê¸°ë³¸ê°’ ë¨¼ì € ì„ ì–¸ (ì¤‘ìš”)
        source_type = "unknown"

        # ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
        save_chat(role="user", content=question)

        answer, source_type = multi_query_rag_with_qt(question, chat_history)

        # AI ì‘ë‹µ ì €ì¥
        save_chat(role="assistant", content=answer)

        return ChatResponse(answer=answer, source_type=source_type)

    except Exception as e:
        print(f"[API ì˜¤ë¥˜] {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "vectordb": "loaded" if vectorstore else "not_loaded",
        "web_search": "enabled" if TAVILY_API_KEY else "disabled"
    }

@app.post("/analyze", response_model=ChatResponse)
async def analyze(request: ChatRequest):
    """
    ì‚¬ì—…ê³„íšì„œ AI ì „ë¬¸ê°€ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸
    - question: ë¶„ì„í•  ì‚¬ì—…ê³„íšì„œ ë‚´ìš©
    - ë°˜í™˜: answer(ë¶„ì„ ê²°ê³¼), source_type("ai-analysis")
    """
    try:
        question = request.question.strip()
        if not question:
            raise HTTPException(status_code=400, detail="ë¶„ì„í•  ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        print(f"\n{'='*60}")
        print(f"[ë¶„ì„ API ìš”ì²­] ì§ˆë¬¸ ê¸¸ì´: {len(question)} ë¬¸ì")
        print(f"{'='*60}")
        
        # ë¶„ì„ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        analysis_system_prompt = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ë²¤ì²˜ íˆ¬ì ì „ë¬¸ê°€ì´ì ì‚¬ì—… ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì‚¬ì—…ê³„íšì„œë¥¼ ì² ì €íˆ ë¶„ì„í•˜ê³  êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤."""
        
        # ê¸°ì¡´ llm ê°ì²´ ì‚¬ìš©
        analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", analysis_system_prompt),
            ("human", "{question}")
        ])
        
        analysis_chain = analysis_prompt | llm | StrOutputParser()
        
        # ë¶„ì„ ì‹¤í–‰
        answer = analysis_chain.invoke({"question": question})
        
        print(f"[ë¶„ì„ API ì‘ë‹µ] ë¶„ì„ ê²°ê³¼ ê¸¸ì´: {len(answer)} ë¬¸ì")
        
        return ChatResponse(answer=answer, source_type="ai-analysis")
        
    except Exception as e:
        print(f"[ë¶„ì„ API ì˜¤ë¥˜] {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ========================================
# ì„œë²„ ì‹¤í–‰
# ========================================
if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš€ ì°½ì—… ì§€ì› AI ì–´ì‹œìŠ¤í„´íŠ¸ API ì„œë²„ ì‹œì‘...")
    print("="*60)
    print(f"ğŸ“š ë²¡í„°DB: {'âœ… ë¡œë“œë¨' if vectorstore else 'âŒ ì—†ìŒ'}")
    print(f"ğŸŒ ì›¹ê²€ìƒ‰: {'âœ… í™œì„±í™”' if TAVILY_API_KEY else 'âŒ ë¹„í™œì„±í™”'}")
    print(f"ğŸ”— ì„œë²„ ì£¼ì†Œ: http://localhost:8000")
    print(f"ğŸ“– API ë¬¸ì„œ: http://localhost:8000/docs")
    print("="*60 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)